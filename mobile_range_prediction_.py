# -*- coding: utf-8 -*-
"""Mobile range prediction .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1taYnT1Pu6vKf_wEQtt4XhovVKwpxJ61G

# **Project Name**    - Mobile price range prediction

##### **Project Type**    - Classification
##### **Contribution**    - Individual

# **Project Summary -**

This is the mobile data and providing the specifiaction of the mobile, and price range of the data. The project summary includes Data preprocessing , Exploratory Data Analysis,Implementation of Machine Learning Models.

# **GitHub Link -**

https://github.com/Junedaktar/Capstone-3--Mobile-range-prediction.git

# **Problem Statement**

**In the competitive mobile phone market companies want to understand sales data of mobile phones and factors which drive the prices.
The objective is to find out some relation between features of a mobile phone(eg:- RAM,Internal Memory, Camera etc) and its selling price. In this problem, we do not have to predict the actual price but a price range indicating how high the price is.**

# ***Let's Begin !***

## ***1. Know Your Data***

### Import Libraries
"""

# Import Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""### Dataset Loading"""

# Load Dataset
df=pd.read_csv('/content/data_mobile_price_range.csv')

"""### Dataset First View"""

# Dataset First Look
df.head()

"""### Dataset Rows & Columns count"""

# Dataset Rows & Columns counts
df.shape

"""### Dataset Information"""

# Dataset Info
df.info()

"""#### Duplicate Values"""

# Dataset Duplicate Value Count
len(df[df.duplicated()])

"""#### Missing Values/Null Values"""

# Missing Values/Null Values Count
df.isnull().sum()

"""## ***2. Understanding Your Variables***"""

# Dataset Columns
df.columns

# Dataset Describe
df.describe()

"""### Variables Description

Battery_power - Total energy a battery can store in one time measured in MAH.

Blue - Has bluetooth or not.

Clock_speed - speed at which microprocessor executes instructions.

Dual_sim - Has dual sim support or not.

Fc - Front Camera mega pixels.

Four_g - Has 4G or not.

Int_memory - Internal Memory in Gigabytes.

M_dep - Mobile Depth in cm.

Mobile_wt - Weight of mobile phone.

N_cores - Number of cores of processor.

Pc - Primary Camera mega pixels.

Px_height - Pixel Resolution Height.

Px_width - Pixel Resolution Width.

Ram - Random Access Memory in Mega Bytes.

Sc_h - Screen Height of mobile in cm.

Sc_w - Screen Width of mobile in cm.

Talk_time - longest time that a single battery charge will last time.

Three_g - Has 3G or not.

Touch_screen - Has touch screen or not.

Wifi - Has wifi or not.

Price_range - This is the target variable with value of 0(low cost),1(medium cost),

2(high cost) and 3(very high cost).

### Check Unique Values for each variable.
"""

# Check Unique Values for each variable.
df['battery_power'].unique()

df['blue'].unique()

df['clock_speed'].unique()

df['dual_sim'].unique()

df['fc'].unique()

df['four_g'].unique()

df['int_memory'].unique()

df['m_dep'].unique()

df['mobile_wt'].unique()

df['n_cores'].unique()

df['pc'].unique()

df['px_height'].unique()

df['px_width'].unique()

df['ram'].unique()

df['sc_h'].unique()

df['talk_time'].unique()

df['sc_w'].unique()

df['three_g'].unique()

df['touch_screen'].unique()

df['wifi'].unique()

df['price_range'].unique()

"""## 3. ***Data Wrangling***

### Data Wrangling Code
"""

# Checking How many observations having sc_w value as 0.
df[df['sc_w']==0]

# Checking How many observations having px_height value as 0.
df[df['px_height']==0]

# Where px_height and sc_w value is 0, assigning them with mean value
df['sc_w'][df['sc_w']==0]=int(df['sc_w'].mean())

df['px_height'][df['px_height']==0]=int(df['px_height'].mean())

df['px_height'][df['px_height']==20]=int(df['px_height'].mean())

"""### What all manipulations have you done and insights you found?

In screen width column have some 0. Screen width cannot be 0, so i replace with the mean of the screen width.
Px_height column have also some row with 0, so i replace with the mean so its perform better for modeling.

## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***

From the above data here it we shows that,
Our dependent variable is price_range and we have four price ranges as target our values.
This dataset is a balanced dataset, i.e., each class has nearly 500 mobile phones.
(0 -> low cost, 1 -> medium cost, 2 -> high cost, 3 -> very high cost).

#### Chart - 1
"""

# lets have look at our target variable's counts
data=df['price_range'].value_counts()

# visualization code
labelling = ['Low-Cost', 'Medium-Cost', 'High-Cost', 'VeryHigh-Cost']
plt.figure(figsize=(10,10))
plt.pie(data,labels=labelling,explode=[0.1,0.1,0.1,0.1],autopct='%.1f%%')

"""##### 1. Why did you pick the specific chart?

In pie chat we can check how the cost data is differ

##### 2. What is/are the insight(s) found from the chart?

Here we shows that count of in each category of price range is similiar .

#### Chart - 2
"""

# Chart - 2 visualization code
plt.figure(figsize=(15,8))
sns.barplot(x=df['price_range'],y=df['ram'])
plt.title('Price range vs Ram')
plt.xlabel('Price range')
plt.ylabel('Ram')

"""##### 1. Why did you pick the specific chart?

Using this Bar plot it shows that how Ram impact on the mobile price range

##### 2. What is/are the insight(s) found from the chart?

Using this Bar plot it shows that as far as RAM specifications such as bytes are increase ,then price range is also increased. As i can check there is positive correlation seems between these variables.

#### Chart - 3
"""

# Chart - 3 visualization code
plt.figure(figsize=(15,10))
sns.barplot(x=df['price_range'],y=df['battery_power'])
plt.title('Price range vs battery power')
plt.xlabel('Price range')
plt.ylabel('Battery power')

"""##### 1. Why did you pick the specific chart?

Bar plot helps to relationship between the two variables

##### 2. What is/are the insight(s) found from the chart?

Mobiles with battery power more than 1300 MAH has very high cost. And Mobiles with battery power between 1200 and 1300 MAH falls under medium and high cost category.

#### Chart - 4
"""

# Chart - 4 visualization code
variables=['sc_h','sc_w','px_height','px_width']
plt.figure(figsize=(20,12))
row=2
column=2
counter=1
for val in variables:
  plt.subplot(row,column,counter)
  sns.barplot(x=df['price_range'],y=df[val])
  plt.title(f'Price range vs {val}')
  plt.xlabel('Price range')
  plt.ylabel(f'{val}')
  counter=counter+1

"""##### 1. Why did you pick the specific chart?

Bar plot helps to relationship between the two variables

##### 2. What is/are the insight(s) found from the chart?

Mobiles with more than 700 pixel height and width more than 1300 width has very high cost. Mobiles screen height and width has some impact on price range.

#### Chart - 5
"""

# Chart - 5 visualization code
variable=['blue','three_g','four_g','wifi','dual_sim','touch_screen']
plt.figure(figsize=(20,15))
row=3
column=2
counter=1

for v in variable:
  plt.subplot(row,column,counter)
  plt.pie(df[v].value_counts(),labels=df[v].unique(),autopct='%.1f%%')
  plt.title(f'{v}')
  counter=counter+1

"""##### 1. Why did you pick the specific chart?

Within pie chat we can check how data is spread with different variables.

##### 2. What is/are the insight(s) found from the chart?

Blue colour indicates it has the specifications,while orange colour indicates it do not have the specifications.
All features has plotted the above pie chart shows nearly 50 percent equality except 3G sim card feature, which shows nearly 76 percent mobile phones who supports the 3g .

#### Chart - 6
"""

# Chart - 6 visualization code
feature=['clock_speed','fc','int_memory','m_dep','mobile_wt','pc','n_cores','talk_time']
for v in feature:
  plt.figure(figsize=(12,8))
  sns.lineplot(x=df['price_range'],y=df[v])
  plt.title(f'Price range vs {v}')
  plt.xlabel('Price range')
  plt.ylabel(f'{v}')

"""#### Chart - 7"""

# Chart - 7 visualization code
plt.figure(figsize=(20,15))
data=df.corr()
sns.heatmap(data,annot=True,cmap='coolwarm')

"""##### 1. Why did you pick the specific chart?

Heat map shows the correlation between the variables

##### 2. What is/are the insight(s) found from the chart?

The above plot shows the correlation of variables among between them.

1. Ram(Random Access Memory) relation with price_range - RAM is the highest correlate with price of mobile phone so,it shows that it is most important and expensive feature respect to remaining all features.

2. fc(Front Camera mega pixels) relation with pc(Primary Camera mega pixels) -The relationship between front and back camera depends on the device design and its intended use, and can vary greatly from one device to another.
If a phone has a great rear camera, it has to match great standards for the front camera as well.

3. Battery power relation wih price of mobile phone - The battery of mobile is slightly positive impact on mobile because ,mobile phone's battery technology can also impact its overall performance and user experience.
Battery is managed by the phone's software and hardware can also affect its performance and lifespan.
Battery_power also has positive correlation with the price range.Generally mobiles having high prices comes with good battery power.

4. latest version four_g(4G) and existing version three_g(3G) - 4g is the latest version of mobile phone, If the phone supports 4g version it means 3g version is already exist in mobile phone and also compatible with this .

5. px_height and px_width -(Pixel Resolution Height and width) are positively correlated with price range.Generally High price range mobiles have good resolutions.

6. The Ram size and Battery power -This shows indirect (Negative) co-relationship ,from the above plot its shows that the ram size and battery power have very less correlate with each other.
The larger the ram size the more power a mobile device will consume.

7. sc_h and sc_w - It shows positively correlated with each other,

8. Mobile weight and its pixel width - It shows exponentially correlate with each other ,it shows that the effect of pixel width on mobile weight is negligible.

9. mobile depth and price range - This features are less correlate with each other it shows that the depth of mobile phone is not significantly effect on its price range as compared to RAM,Battery power,Display resolution ,camera .

Answer Here

## ***5. Feature Engineering & Data Pre-processing***

### 1. Handling Missing Values
"""

# Handling Missing Values & Missing Value Imputation
df.isnull().sum()

"""### 2. Handling Outliers"""

# Handling Outliers & Outlier treatments
feature=['battery_power','clock_speed', 'fc', 'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height','px_width', 'ram', 'sc_h', 'sc_w', 'talk_time']
for f in feature:
  plt.figure(figsize=(10,8))
  plt.boxplot(df[f])
  plt.title(f'{f}')

"""##### Removing the Outliers """

Q1 = df["fc"].quantile(0.25)
Q3 = df['fc'].quantile(0.99)
Inter_Quartile_Range = Q3-Q1

# Outliers are present after Quartile 3. so we will take datapoints before Q3.
df= df[(df['fc'] <= Q3)]

Q1 = df["px_height"].quantile(0.25)
Q3 = df['px_height'].quantile(0.99)
IQR = Q3-Q1

# Outliers are present after Quartile 3. so we will take datapoints before Q3.
df = df[(df['px_height'] <= Q3)]

"""##### What all outlier treatment techniques have you used and why did you use those techniques?

There is no such outliers in our data set. Only front camera and px height have some outliers so we remove some data after 99 percentile.

### 4. Feature Manipulation & Selection

#### 1. Feature Manipulation
"""

# Manipulate Features to minimize feature correlation and create new features
df['pixel']=df['px_height']*df['px_width']
df['sc_size']=round(np.sqrt((df['sc_h']**2)+(df['sc_w']**2)),2)

df.drop(columns=['px_height','px_width'],axis=1,inplace=True)
df.drop(columns=['sc_h','sc_w'],axis=1,inplace=True)

"""### 2. Data Scaling"""

# Scaling your data
dependent='price_range'
independent=list(set(df.columns.tolist())-{dependent})

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
df[independent]=scaler.fit_transform(df[independent])

df[independent].head()

"""##### Which method have you used to scale you data and why?

### 3. selcting the data
"""

x=df[independent].values
y=df[dependent].values

"""### 4. Data Splitting"""

# Split your data to train and test. Choose Splitting ratio wisely.
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)
print(x_train.shape)
print(x_test.shape)

"""### 5. Handling Imbalanced Dataset"""

df['price_range'].value_counts()

"""##### Do you think the dataset is imbalanced? Explain Why.

As i can check all the outputs are equal so data is purely balanced

## ***7. ML Model Implementation***

### Logistic Regression
"""

# ML Model - 1 Implementation
from sklearn.linear_model import LogisticRegression
classifier=LogisticRegression(fit_intercept=True,max_iter=1000)
# Fit the Algorithm
classifier.fit(x_train,y_train)
# Predict on the model
train_pred=classifier.predict(x_train)
test_pred=classifier.predict(x_test)

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

#  Evaluation Metric Score chart- Accuracy score
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
accuracy_score(train_pred,y_train)

accuracy_score(test_pred,y_test)

# Evaluation Metric Score chart- Confusion matrix
cm_train=confusion_matrix(train_pred,y_train)
cm_train

cm_test=confusion_matrix(test_pred,y_test)
cm_test

# Ploting the confusion matrix
ax=sns.heatmap(cm_test,annot=True,cmap='Blues')
ax.set_title('Confusion Matrix')
ax.set_xlabel('Actual value')
ax.set_ylabel('Predicted value')
ax.xaxis.set_ticklabels([0,1,2,3])
ax.yaxis.set_ticklabels([0,1,2,3])
plt.show()

#  Evaluation Metric Score chart- Classification matrix
print(classification_report(train_pred,y_train))

print(classification_report(test_pred,y_test))

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)
from sklearn.model_selection import GridSearchCV
parameter={'penalty':['l1','l2','elasticnet'],'C':[1,2,3,5,8,10,20,30,40,50],'max_iter':[100,200,300,500,1000]}
classifier_regressor=GridSearchCV(classifier,param_grid=parameter,scoring='accuracy',cv=5)
# Fit the Algorithm
classifier_regressor.fit(x_train,y_train)
# Predict on the model
train_cv_pred=classifier_regressor.predict(x_train)
test_cv_pred=classifier_regressor.predict(x_test)

"""####3.Logistic regression model with Cross validation evaluation matrix"""

#  Evaluation Metric Score chart- Accuracy score
accuracy_score(train_cv_pred,y_train)

accuracy_score(test_cv_pred,y_test)

#  Evaluation Metric Score chart- Confusion matrix
cm_train=confusion_matrix(train_cv_pred,y_train)
cm_train

cm_test=confusion_matrix(test_cv_pred,y_test)
cm_test

# Ploting the confusion matrix
ax=sns.heatmap(cm_test,annot=True,cmap='Blues')
ax.set_title('Confusion Matrix')
ax.set_xlabel('Actual value')
ax.set_ylabel('Predicted value')
ax.xaxis.set_ticklabels([0,1,2,3])
ax.yaxis.set_ticklabels([0,1,2,3])
plt.show()

#  Evaluation Metric Score chart- Classification report
print(classification_report(train_cv_pred,y_train))

print(classification_report(test_cv_pred,y_test))

"""##### Which hyperparameter optimization technique have you used and why?

I used the grid search cross validation and hyperparamer tunning to predict the better accuracy

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

Before Tuning training accuarcy = 94%, test accuarcy = 91%
Model is overfitted the data and does not generalised well. So we tuned the hyperparameters.
After tuning: Training accuarcy= 95%, Test accuarcy = 92.5%
we have slightly improved the model and overfitting is reduced slightly.

### Decision tree
"""

# ML Model - 2 Implementation
from sklearn.tree import DecisionTreeClassifier
tree_classifier=DecisionTreeClassifier(criterion='entropy',max_leaf_nodes=10,random_state=0)
# Fit the Algorithm
tree_classifier.fit(x_train,y_train)

# Predict the model
train_tree_pred=tree_classifier.predict(x_train)
test_tree_pred=tree_classifier.predict(x_test)

#Plot the decision tree
from sklearn import tree
plt.figure(figsize=(20,12))
tree.plot_tree(tree_classifier,filled=True)

"""#### 1.  Model performance using Evaluation metric Score."""

# Evaluation Metric Score chart- Accuracy
accuracy_score(train_tree_pred,y_train)

accuracy_score(test_tree_pred,y_test)

# Evaluation Metric Score chart- Confusion matrix
cm_train=confusion_matrix(train_tree_pred,y_train)
cm_train

# Ploting the confusion matrix
ax=sns.heatmap(cm_train,annot=True,cmap='Blues')
ax.set_title('Confusion Matrix')
ax.set_xlabel('Actual value')
ax.set_ylabel('Predicted value')
ax.xaxis.set_ticklabels([0,1,2,3])
ax.yaxis.set_ticklabels([0,1,2,3])
plt.show()

cm_test=confusion_matrix(test_tree_pred,y_test)
cm_test

# Ploting the confusion matrix
ax=sns.heatmap(cm_test,annot=True,cmap='Blues')
ax.set_title('Confusion Matrix')
ax.set_xlabel('Actual value')
ax.set_ylabel('Predicted value')
ax.xaxis.set_ticklabels([0,1,2,3])
ax.yaxis.set_ticklabels([0,1,2,3])
plt.show()

# Evaluation Metric Score chart- classification report
print(classification_report(train_tree_pred,y_train))

print(classification_report(test_tree_pred,y_test))

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)
grid={'max_depth':[1,2,3,4,5,6,7,8,9,10,15,20],"min_samples_leaf":[1,2,3,4,5,10,15,20,40,60,80,100],"max_leaf_nodes":[10,20,30,40,50,70,100,140,150]}
class_model=DecisionTreeClassifier()
tree_classifier=GridSearchCV(class_model,param_grid=grid,cv=7)
# Fit the Algorithm
tree_classifier.fit(x_train,y_train)
# Predict on the model
train_tree_cv_pred=tree_classifier.predict(x_train)
test_tree_cv_pred=tree_classifier.predict(x_test)

# Best parameter
tree_classifier.best_params_

"""####3.Decision tree model with cross validation performance using Evaluation metric"""

# Evaluation Metric Score chart- Accuracy score
accuracy_score(train_tree_cv_pred,y_train)

accuracy_score(test_tree_cv_pred,y_test)

# Evaluation Metric Score chart- Confusion matrix
confusion_matrix(train_tree_cv_pred,y_train)

cm_cv_test=confusion_matrix(test_tree_cv_pred,y_test)
cm_cv_test

# Ploting the confusion matrix
ax=sns.heatmap(cm_cv_test,annot=True,cmap='Blues')
ax.set_title('Confusion Matrix')
ax.set_xlabel('Actual value')
ax.set_ylabel('Predicted value')
ax.xaxis.set_ticklabels([0,1,2,3])
ax.yaxis.set_ticklabels([0,1,2,3])
plt.show()

# Evaluation Metric Score chart- Classification report
print(classification_report(train_tree_cv_pred,y_train))

print(classification_report(test_tree_cv_pred,y_test))

"""##### Which hyperparameter optimization technique have you used and why?

I used the grid search cross validation and hyperparamer tunning to predict the better accuracy. Previously may have the overfiting the data.

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

Before Tuning training accuarcy = 79%, test accuarcy = 78%
Model is overfitted the data and does not generalised well. So we tuned the hyperparameters.
After tuning: Training accuarcy= 94%, Test accuarcy = 87.2%
we have slightly improved the model and overfitting is reduced slightly.

### Random Forest
"""

# ML Model - 3 Implementation
from sklearn.ensemble import RandomForestClassifier
classification=RandomForestClassifier(random_state=0)
# Fit the Algorithm
classification.fit(x_train,y_train)
# Predict on the model
train_random_pred=classification.predict(x_train)
test_random_pred=classification.predict(x_test)

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

#  Evaluation Metric Score chart- Accuracy score
accuracy_score(train_random_pred,y_train)

accuracy_score(test_random_pred,y_test)

#  Evaluation Metric Score chart- Confusion matrix
confusion_matrix(train_random_pred,y_train)

cm_test=confusion_matrix(test_random_pred,y_test)
cm_test

# Ploting the confusion matrix
ax=sns.heatmap(cm_test,annot=True,cmap='Blues')
ax.set_title('Confusion Matrix')
ax.set_xlabel('Actual value')
ax.set_ylabel('Predicted value')
ax.xaxis.set_ticklabels([0,1,2,3])
ax.yaxis.set_ticklabels([0,1,2,3])
plt.show()

#  Evaluation Metric Score chart- Clasification report
print(classification_report(train_random_pred,y_train))

print(classification_report(test_random_pred,y_test))

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)
grid_parameter={'criterion':['gini','entropy'],'n_estimators':[10,20,30,40,50],'max_depth':[3,5,7,10],
                'min_samples_split':[2,4,6],'max_leaf_nodes':[None],'max_features':['auto','log2','sqrt']}
rfc=RandomForestClassifier(random_state=0)
# Fit the Algorithm
rfc_grid=GridSearchCV(rfc,param_grid=grid_parameter,scoring='accuracy',cv=7,verbose=5)
rfc_grid.fit(x_train,y_train)
# Predict on the model
train_random_cv_pred=rfc_grid.predict(x_train)
test_random_cv_pred=rfc_grid.predict(x_test)

"""####3.Random forest model with cross validation performance using Evaluation metric"""

# Best parameter
rfc_grid.best_params_

#  Evaluation Metric Score chart- Accuracy score
accuracy_score(train_random_cv_pred,y_train)

accuracy_score(test_random_cv_pred,y_test)

#  Evaluation Metric Score chart- Confusion matrix
confusion_matrix(train_random_cv_pred,y_train)

cm_test=confusion_matrix(test_random_cv_pred,y_test)
cm_test

# Ploting the confusion matrix report
ax=sns.heatmap(cm_test,annot=True,cmap='Blues')
ax.set_title('Confusion Matrix')
ax.set_xlabel('Actual value')
ax.set_ylabel('Predicted value')
ax.xaxis.set_ticklabels([0,1,2,3])
ax.yaxis.set_ticklabels([0,1,2,3])
plt.show()

# Evalution martrix score chart- Classification report
print(classification_report(train_random_cv_pred,y_train))

print(classification_report(test_random_cv_pred,y_test))

"""##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

No after used the cross validation its didnot perform well.

### Support vector machine
"""

# ML Model - 4 Implementation
from sklearn import svm
svm_classifier=svm.SVC(probability=True)
# Fit the Algorithm
svm_classifier.fit(x_train,y_train)
# Predict on the model
train_svm_pred=svm_classifier.predict(x_train)
test_svm_pred=svm_classifier.predict(x_test)

#  Evaluation Metric Score chart- Accuracy score
accuracy_score(train_svm_pred,y_train)

accuracy_score(test_svm_pred,y_test)

#  Evaluation Metric Score chart- Confusion matrix
cm_train=confusion_matrix(train_svm_pred,y_train)
cm_train

cm_test=confusion_matrix(test_svm_pred,y_test)
cm_test

# Ploting the confusion matrix
ax=sns.heatmap(cm_test,annot=True,cmap='Blues')
ax.xaxis.set_ticklabels([0,1,2,3])
ax.yaxis.set_ticklabels([0,1,2,3])
plt.show()

"""### Comparision of all the models"""

# Creating a dictionary
comparision_dict={'Model_name':['Logistic regression','Decision tree','Random Forest','Support vector machine'],
                  'Training_accuracy':[0.95,0.94,0.995,0.97],'Test_accuracy':[0.93,0.87,0.85,0.87]}

# Creating a dataframe
comparision_df=pd.DataFrame(comparision_dict)

comparision_df

# Ploting the traing accuracy of different models
plt.figure(figsize=(10,5))
sns.barplot(data=comparision_df, x='Model_name', y='Training_accuracy', palette='ch:s=-.2,r=.6')

# Ploting the test accuracy of different models
plt.figure(figsize=(10,5))
sns.barplot(data=comparision_df, x='Model_name', y='Test_accuracy', palette='ch:s=-.2,r=.6')

"""### 1. Which Evaluation metrics did you consider for a positive business impact and why?

We used different type of evaluation matrix such as accuracy score, confusion matrix and classification report. According to business you can check which matrix is important. 
Accuracy score shows the how accurate the prediction model is. And confusion matrix give the clear image of the data which includes the true positive, true negetive, false positive, false negetive. 
Classification report show the precision and recall in which you can consider for which is important for business point of view.

### 2. Which ML model did you choose from the above created models as your final prediction model and why?

I have created the different models such as logistic regression, decision tree, random forest and support vector machine. But as we check better model performance of logistic regression.

# **Conclusion**

From all the above graphical visualisation shows that Logistic Regression is best model for production purpose.
As observed Logistic Regression and decision tree model performs well along with support vector machine .
Out of all the model Logistic Regression is perform well on overall basis.
Ram, Battery power, Mobile weight, Screen size and Pixels are key features in predicting the mobile price range.

### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***
"""